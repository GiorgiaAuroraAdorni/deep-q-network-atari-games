\documentclass[a4paper,12pt]{article} % This defines the style of your paper

\usepackage[top = 2.5cm, bottom = 2.5cm, left = 2.5cm, right = 2.5cm]{geometry} 
\usepackage[utf8]{inputenc} %utf8 % lettere accentate da tastiera
\usepackage[english]{babel} % lingua del documento
\usepackage[T1]{fontenc} % codifica dei font

\usepackage{multirow} % Multirow is for tables with multiple rows within one 
%cell.
\usepackage{booktabs} % For even nicer tables.

\usepackage{graphicx} 

\usepackage{setspace}
\setlength{\parindent}{0in}

\usepackage{float}

\usepackage{fancyhdr}

\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{color}

\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{subfigure}

\usepackage{ifxetex,ifluatex}
\usepackage{etoolbox}
\usepackage[svgnames]{xcolor}

\usepackage{tikz}

\usepackage{framed}

\usepackage{numprint}

 \newcommand*\quotefont{\fontfamily{LinuxLibertineT-LF}} % selects Libertine as 
 %the quote font


\newcommand*\quotesize{40} % if quote size changes, need a way to make shifts 
%relative
% Make commands for the quotes
\newcommand*{\openquote}
{\tikz[remember picture,overlay,xshift=-4ex,yshift=-1ex]
	\node (OQ) 
	{\quotefont\fontsize{\quotesize}{\quotesize}\selectfont``};\kern0pt}

\newcommand*{\closequote}[1]
{\tikz[remember picture,overlay,xshift=4ex,yshift=-1ex]
	\node (CQ) {\quotefont\fontsize{\quotesize}{\quotesize}\selectfont''};}

% select a colour for the shading
\colorlet{shadecolor}{WhiteSmoke}

\newcommand*\shadedauthorformat{\emph} % define format for the author argument

% Now a command to allow left, right and centre alignment of the author
\newcommand*\authoralign[1]{%
	\if#1l
	\def\authorfill{}\def\quotefill{\hfill}
	\else
	\if#1r
	\def\authorfill{\hfill}\def\quotefill{}
	\else
	\if#1c
	\gdef\authorfill{\hfill}\def\quotefill{\hfill}
	\else\typeout{Invalid option}
	\fi
	\fi
	\fi}
% wrap everything in its own environment which takes one argument (author) and 
%one optional argument
% specifying the alignment [l, r or c]
%
\newenvironment{shadequote}[2][l]%
{\authoralign{#1}
	\ifblank{#2}
	{\def\shadequoteauthor{}\def\yshift{-2ex}\def\quotefill{\hfill}}
	{\def\shadequoteauthor{\par\authorfill\shadedauthorformat{#2}}\def\yshift{2ex}}
	\begin{snugshade}\begin{quote}\openquote}
		{\shadequoteauthor\quotefill\closequote{\yshift}\end{quote}\end{snugshade}}

\newcommand{\footref}[1]{%
	$^{\ref{#1}}$%
}
\newcommand{\footlabel}[2]{%
	\addtocounter{footnote}{1}%
	\footnotetext[\thefootnote]{%
		\addtocounter{footnote}{-1}%
		\refstepcounter{footnote}\label{#1}%
		#2%
	}%
	$^{\ref{#1}}$%
}


\pagestyle{fancy}

\setlength\parindent{24pt}

\fancyhf{}

\lhead{\footnotesize Deep Learning Lab: Assignment 4}

\rhead{\footnotesize Giorgia Adorni}

\cfoot{\footnotesize \thepage} 

\begin{document}
	\thispagestyle{empty}  
	\noindent{
	\begin{tabular}{p{15cm}} 
		{\large \bf Deep Learning Lab} \\
		Universit√† della Svizzera Italiana \\ Faculty of Informatics \\ \today  \\
		\hline
		\\
	\end{tabular} 
	
	\vspace*{0.3cm} 
	
	\begin{center}
		{\Large \bf Assignment 4: Deep Q-Network}
		\vspace{2mm}
		
		{\bf Giorgia Adorni (giorgia.adorni@usi.ch)}
		
	\end{center}  
}
	\vspace{0.4cm}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Introduction}
	\label{section:intro}
	
	The goal of this project is to implement and train a Deep Q-Network agent, based on \textit{Mnih et al., 2015}.
	
	All the models were implemented using TensorFlow and trained on an NVIDIA Tesla V100-PCIE-16GB GPU.
	
	\section{Environment, Agent and Training}
	\label{section:agent}	
	\texttt{OpenAI Gym} has been used to create the \textit{BreakoutNoFrameskip-v4} environment. 
	
	The DQN agent has three main components: an online Q-network, a target Q-network and replay buffer.
	The two networks are used to improve the stability of this method, in particular, every $C$ steps, the target network is updated with the online network parameters.
	
	The replay buffer of capacity $\numprint{10000}$ is composed of state, action, reward, next state, and termination flag. It will be used during the training in order to create batches by sampling them from the buffer.
	
	In Table \ref{tab:arc} is summarised the architecture of both the networks. 
	
	\begin{figure}[htb]
		\centering
		
		\begin{tabular}{ccccc}
			\toprule
			\textbf{conv1} & \textbf{conv2} & \textbf{conv3} & \textbf{fc1} &
			\textbf{fc2} \\
			\midrule
			8$\times$8,  32 & 4$\times$4, 64 & 3$\times$3, 64 & 512 & k\\
			s. 4$\times$4 &   s. 2$\times$2 &   s. 1$\times$1 &  & \\
			p. same & p. same & p. same &&\\
			ReLU & ReLU & ReLU & ReLU & ReLU  \\
			\bottomrule
		\end{tabular}
		\captionof{table}{Network architecture}
		\label{tab:arc}
	\end{figure}
	
	\section{Tasks}
	\label{section:tasks}
	
	\subsection*{Wrappers}
	\begin{itemize}
		\item The \texttt{FrameStack} wrappers is used to stack the last $k$ frames returning a lazy array, which is a structure much more memory efficient.
		\item The \texttt{ScaledFloatFrame} wrapper rescales the value of the pixels initially comprised between 0 and 255 between 0-1.
		\item The \texttt{MaxAndSkipEnv} wrapper is used to reduce the number of frame, hence the quantity of data to process. In order to do this, firstly it skips some frames of the game play. Then, for all the skipped frames, it continually repeats the same action, sums the rewards in order not to lose information and at the end, for each pixel of the frame, the maximum pixel value is chosen among all the frames skipped.
		\item The \texttt{ClipRewardEnv} wrapper classifies the reward as $+1$, $0$ or $-1$ according its sign.
	\end{itemize}
	
	
	\subsection*{Online Q-network and Target Q-network}	
	As mention in Section \ref{section:agent}, the addition of the target network, the this used to generate targets in the Q-learning update, improves the stability of the presented method. In particular, every $C$ steps, the target network is updated with the online network parameters.
	This procedure adds a delay between the time between the time the network is updated and the time when the update affects the target, making divergence or oscillations much more unlikely.
	
	\subsection*{$\epsilon$-greedy policy}
	%FIXME
	
	Acting according to an $\epsilon$-greedy policy ensure an adequate exploration of the environment in order to learn about potentially (could be better or worse) new sources of reward, instead of exploit the well-known sources of reward.
	
	For a given state $s$, the $\epsilon$-greedy policy with respect to Q chooses a random action with probability $\epsilon$, and an action $\arg \max_a Q(s, a)$ with probability $1 - \epsilon$.
	
	\section{Experiments}
	\subsection*{Experiment 1}
	The first experiment includes a training phase in which the agent interact with the environment for a total of $\numprint{2000000}$ steps.
	
	The replay buffer is initially empty, and the networks are not updated until it is populated with $\numprint{10000}$ transitions. 
	
	Every 4 steps, a batch composed 32 transitions is sampled from the replay buffer and used to update the parameters of the online Q-network.
	Root mean square prop (\texttt{RMSPropOptimizer}) is used as optimiser to minimise the temporal-difference error, with learning rate of ${0.0001}$ and a decay of $0.99$. 
	
	The parameters of the online network are copied to the target network every $\numprint{10000}$ steps.
	\bigskip

	Figure \ref{fig:step-m1} shows the number of steps elapsed in each episode averaged over the last 100 episodes.
	\begin{figure}[htb]
		\centering
		\includegraphics[width=.7\linewidth]{../code/out/m1/img/step-per-episode.pdf}	
		\caption{Steps per episode of the first experiment}
		\label{fig:step-m1}
	\end{figure}

	It is clearly visible how the number of steps for episode increase over time. Since each episode corresponds to a "life of play", a greater number of steps per episode can be interpreted as an improvement in the network's ability to play the game without losing a life quickly. 
	\bigskip
	
	Figures \ref{fig:training-return-m1} shows the return per episode, averaged over the last 30 episodes (moving average) to reduce noise, while Figure \ref{fig:evaluation-return-m1} shows scores across 30 independent plays, that correspond to the sum of the return obtained across a sequence of 5 different episodes.
	
	\begin{figure}[htb]
		\begin{minipage}[b]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../code/out/m1/img/training-return.pdf}	
			\caption{Return per episode}
			\label{fig:training-return-m1}
		\end{minipage}
		~
		\begin{minipage}[b]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../code/out/m1/img/evaluation-return.pdf}	
			\caption{Average score per play}
			\label{fig:evaluation-return-m1}
		\end{minipage}
	\end{figure}

	The game score obtained is $358.0$ that is, as expected, a little lower compared to the one achieved in literature, that is $401.2 \, (\pm 26.9)$, since the model presented in the paper is much more complex.
	Moreover, the trend of the curve is growing, which means that further increasing the number of steps, the score should continue to improve.
	\bigskip
		
	In Figure \ref{fig:loss-m1} is shown a subsample of the temporal-difference error L($\theta$) averaged over the last 50 steps in order to reduce noise.
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=.7\linewidth]{../code/out/m1/img/temporal-difference-error.pdf}	
		\caption{Loss of the first experiment}
		\label{fig:loss-m1}
	\end{figure} 

	\bigskip
	The training process takes 5 hours and 18 minutes.
	
	\subsection*{Experiment 2}
	The second experiment performed modifies only the value of the parameter $C$ with respect to the previous experiment. In particular, the target network is updated every $\numprint{50000}$ instead of $\numprint{10000}$.
	
	The training process takes 4 hours. Figure \ref{fig:score-m1-m2} shows a comparison among the evaluation scores of the experiments.
	
	\begin{figure}[htb]
		\centering
		\includegraphics[width=.7\linewidth]{../code/out/m2/img/comparison/scores-comparison-m1-m2.pdf}	
		\caption{Score comparison}
		\label{fig:score-m1-m2}
	\end{figure}
	It is clearly visible how, by reducing the frequency of the updates, the network slows down the learning.
	
	\subsection*{Experiment 3}
	The third experiment repeats the same training procedure as the first experiment using a different environment. The Atari game tried in this case is \textit{StarGunner}. The training process takes 4 hours and 11 minutes.
	
	Figures \ref{fig:return-movingavg-m3} and \ref{fig:score-m3} show the return per episode (moving average) and scores across 30 independent plays.
	
	\begin{figure}[htb]
		\begin{minipage}[b]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../code/out/m3/img/training-return.pdf}	
			\caption{Return per episode}
			\label{fig:return-movingavg-m3}
		\end{minipage}
		~
		\begin{minipage}[b]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../code/out/m3/img/evaluation-return.pdf}	
			\caption{Average score per play}
			\label{fig:score-m3}
		\end{minipage}
	\end{figure}
	
	Also in this case, the return curves have a growing trend. After $1.5 M$ steps the score increases very quickly, with a peak at $3020.0$ and then slightly decreases around $2 M$ steps.
	\bigskip
	
	In Figure \ref{fig:loss-m3} is shown a subsample of the temporal-difference error L($\theta$) averaged over the last 50 steps.
	\begin{figure}[htb]
		\centering
		\includegraphics[width=.7\linewidth]{../code/out/m3/img/temporal-difference-error.pdf}	
		\caption{Loss of the third experiment}
		\label{fig:loss-m3}
	\end{figure} 
	
	In this case, the loss does not have a low value nor a decreasing trend as for the first experiment.
	
	%Figures \ref{fig:score-comparison-m1-m3} and \ref{fig:score-comparison-m2-m3} FIXME
%	
%		\begin{figure}[htb]
%		\begin{minipage}[b]{.49\textwidth}
%			\centering
%			\includegraphics[width=\linewidth]{../code/out/m3/img/comparison/scores-comparison-m1-m3.pdf}	
%			\caption{Return per episode}
%			\label{fig:score-comparison-m1-m3}
%		\end{minipage}
%		~
%		\begin{minipage}[b]{.49\textwidth}
%			\centering
%			\includegraphics[width=\linewidth]{../code/out/m3/img/comparison/scores-comparison-m2-m3.pdf}	
%			\caption{Average score per play}
%			\label{fig:score-comparison-m2-m3}
%		\end{minipage}
%	\end{figure}

	
	\subsection*{Experiment 4}
	The last experiment tries to improve the results for \texttt{Breakout} obtained in the first experiment, using the data recorded from $\numprint{10000}$ gameplay steps to populate the replay buffer.
	In this case the model is trained for $\numprint{300000}$ steps and the process takes 31 minutes.
	\bigskip

	Figures \ref{fig:return-movingavg-m4} and \ref{fig:score-m4} show the return per episode (moving average) and scores across 30 independent plays
	
	\begin{figure}[h]
		\begin{minipage}[b]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../code/out/m4/img/training-return.pdf}	
			\caption{Return per episode}
			\label{fig:return-movingavg-m4}
		\end{minipage}
		~
		\begin{minipage}[b]{.49\textwidth}
			\centering
			\includegraphics[width=\linewidth]{../code/out/m4/img/evaluation-return.pdf}	
			\caption{Average score per play}
			\label{fig:score-m4}
		\end{minipage}
	\end{figure}

	\begin{figure}[htb]
		\centering
		\includegraphics[width=.7\linewidth]{../code/out/m4/img/comparison/scores-comparison-m1-m4.pdf}	
		\caption{Score comparison}
		\label{fig:score-m1-m4}
	\end{figure}
	
	The evaluation return obtained during this experiment is $14.33$, that is similar to the one obtained in the first experiment at the same time step, that is $15.03$.
	
	The similar results are probably due the limited capacity of the replay buffer. In fact, after only $\numprint{10000}$ steps, the past data experiences are replaced.
	
	
\end{document}
